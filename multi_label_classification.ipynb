{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592db6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/strongio/keras-bert/blob/master/keras-bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370126f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e5deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64238e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_URL = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
    "module = hub.Module(BERT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202594a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d00657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        return input_ids, input_mask, segment_ids\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids = [], [], []\n",
    "    for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8877728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric(y_true, y_pred):\n",
    "    n = tf.size(y_true)\n",
    "    eq = tf.math.equal(y_true, y_pred)\n",
    "    eq = tf.reduce_sum(tf.cast(eq, tf.int32))\n",
    "    return eq * 100 / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31312bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length, num_classes):\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)\n",
    "#     lstm_1 = tf.keras.layers.LSTM(256, return_sequences=True)(bert_output)\n",
    "#     dropout_1 = tf.keras.layers.Dropout(0.2)(lstm_1)\n",
    "#     lstm_2 = tf.keras.layers.LSTM(128, return_sequences=True)(bert_output)\n",
    "#     dropout_2 = tf.keras.layers.Dropout(0.2)(lstm_2)\n",
    "#     lstm_3 = tf.keras.layers.LSTM(64)(dropout_2)\n",
    "#     dense_1 = tf.keras.layers.Dense(50, activation='relu')(lstm_3)\n",
    "    dense_2 = tf.keras.layers.Dense(num_classes, activation=\"sigmoid\")(bert_output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=dense_2)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\", my_metric])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba303b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"mean\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'output_size': self.output_size,\n",
    "            'pooling': self.pooling,\n",
    "            'n_fine_tune_layers': self.n_fine_tune_layers,\n",
    "            'trainable': self.trainable\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b032d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         n_fine_tune_layers=10,\n",
    "#         pooling=\"mean\",\n",
    "#         bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         self.n_fine_tune_layers = n_fine_tune_layers\n",
    "#         self.trainable = True\n",
    "#         self.output_size = 768\n",
    "#         self.pooling = pooling\n",
    "#         self.bert_path = bert_path\n",
    "#         if self.pooling not in [\"first\", \"mean\"]:\n",
    "#             raise NameError(\n",
    "#                 f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "#             )\n",
    "\n",
    "#         super(BertLayer, self).__init__(**kwargs)\n",
    "        \n",
    "#     def get_config(self):\n",
    "\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'output_size': self.output_size,\n",
    "#             'pooling': self.pooling,\n",
    "#             'n_fine_tune_layers': self.n_fine_tune_layers,\n",
    "#             'trainable': self.trainable\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.bert = hub.Module(\n",
    "#             self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "#         )\n",
    "\n",
    "#         # Remove unused layers\n",
    "#         trainable_vars = self.bert.variables\n",
    "#         if self.pooling == \"first\":\n",
    "#             trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "#             trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "#         elif self.pooling == \"mean\":\n",
    "#             trainable_vars = [\n",
    "#                 var\n",
    "#                 for var in trainable_vars\n",
    "#                 if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "#             ]\n",
    "#             trainable_layers = []\n",
    "#         else:\n",
    "#             raise NameError(\n",
    "#                 f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "#             )\n",
    "\n",
    "#         # Select how many layers to fine tune\n",
    "#         for i in range(self.n_fine_tune_layers):\n",
    "#             trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "#         # Update trainable vars to contain only the specified layers\n",
    "#         trainable_vars = [\n",
    "#             var\n",
    "#             for var in trainable_vars\n",
    "#             if any([l in var.name for l in trainable_layers])\n",
    "#         ]\n",
    "\n",
    "#         # Add to trainable weights\n",
    "#         for var in trainable_vars:\n",
    "#             self._trainable_weights.append(var)\n",
    "\n",
    "#         for var in self.bert.variables:\n",
    "#             if var not in self._trainable_weights:\n",
    "#                 self._non_trainable_weights.append(var)\n",
    "\n",
    "#         super(BertLayer, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "#         input_ids, input_mask, segment_ids = inputs\n",
    "#         bert_inputs = dict(\n",
    "#             input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "#         )\n",
    "#         result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "#             \"sequence_output\"\n",
    "#         ]\n",
    "#         return result\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f77bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(bert_path):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module = hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9567c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_examples(texts):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text in texts:\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None)\n",
    "        )\n",
    "    return InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ae7e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f81d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2597ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ada2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\Anna\\Files\\SentimentAnalysis\\ugam\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4925d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6136, 14)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07f7f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = train_test_split(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79b8e32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5522, 14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4660f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15e54522",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['Review'].tolist()\n",
    "train_text = [\" \".join(t.split()[0:max_seq_length]) for t in train_text]\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "\n",
    "dev_text = dev['Review'].tolist()\n",
    "dev_text = [\" \".join(t.split()[0:max_seq_length]) for t in dev_text]\n",
    "dev_text = np.array(dev_text, dtype=object)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a491da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(['Components', 'Delivery and Customer Support',\n",
    "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
    "       'Installation', 'Material', 'Price', 'Quality', 'Usability',\n",
    "       'Polarity'])\n",
    "train_labels = train[['Components', 'Delivery and Customer Support',\n",
    "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
    "       'Installation', 'Material', 'Price', 'Quality', 'Usability',\n",
    "       'Polarity']].to_numpy()\n",
    "dev_labels = dev[['Components', 'Delivery and Customer Support',\n",
    "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
    "       'Installation', 'Material', 'Price', 'Quality', 'Usability',\n",
    "       'Polarity']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f37367f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/2175286463.py:2: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/2175286463.py:2: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ac0d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\anna\\pycharmprojects\\ugam_sentiment\\venv\\lib\\site-packages\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\anna\\pycharmprojects\\ugam_sentiment\\venv\\lib\\site-packages\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "tokenizer = create_tokenizer_from_hub_module(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25f6d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text)\n",
    "dev_examples = convert_text_to_examples(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc96f9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5522/5522 [00:02<00:00, 2604.98it/s]\n",
      "Converting examples to features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 614/614 [00:00<00:00, 2355.93it/s]\n"
     ]
    }
   ],
   "source": [
    "(train_input_ids,\n",
    "train_input_masks,\n",
    "train_segment_ids) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(dev_input_ids,\n",
    "dev_input_masks,\n",
    "dev_segment_ids) = convert_examples_to_features(tokenizer, dev_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "358cdabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5522, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f7733f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5522, 12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0d2d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                min_delta=0,\n",
    "                                patience=3,\n",
    "                                verbose=0,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e8604e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\anna\\pycharmprojects\\ugam_sentiment\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\anna\\pycharmprojects\\ugam_sentiment\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\anna\\pycharmprojects\\ugam_sentiment\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\anna\\pycharmprojects\\ugam_sentiment\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 12)           9228        bert_layer[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 110,114,118\n",
      "Trainable params: 21,272,844\n",
      "Non-trainable params: 88,841,274\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_seq_length, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2259829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/3640279717.py:2: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/3640279717.py:2: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/3640279717.py:3: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/3640279717.py:3: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/3640279717.py:4: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/3640279717.py:4: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/3640279717.py:5: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anna\\AppData\\Local\\Temp/ipykernel_24024/3640279717.py:5: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate variables\n",
    "initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4975ffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5522 samples, validate on 614 samples\n",
      "Epoch 1/20\n",
      "5522/5522 [==============================] - 1397s 253ms/sample - loss: 0.2189 - categorical_accuracy: 0.2300 - my_metric: 0.0000e+00 - val_loss: 0.1940 - val_categorical_accuracy: 0.3925 - val_my_metric: 0.0000e+00\n",
      "Epoch 2/20\n",
      "5522/5522 [==============================] - 1393s 252ms/sample - loss: 0.1400 - categorical_accuracy: 0.3182 - my_metric: 0.0000e+00 - val_loss: 0.1807 - val_categorical_accuracy: 0.2427 - val_my_metric: 0.0000e+00\n",
      "Epoch 3/20\n",
      "5522/5522 [==============================] - 1398s 253ms/sample - loss: 0.1116 - categorical_accuracy: 0.3729 - my_metric: 0.0000e+00 - val_loss: 0.1732 - val_categorical_accuracy: 0.2785 - val_my_metric: 0.0000e+00\n",
      "Epoch 4/20\n",
      "5522/5522 [==============================] - 1400s 253ms/sample - loss: 0.0864 - categorical_accuracy: 0.3933 - my_metric: 0.0000e+00 - val_loss: 0.2007 - val_categorical_accuracy: 0.2199 - val_my_metric: 0.0000e+00\n",
      "Epoch 5/20\n",
      "5522/5522 [==============================] - 1392s 252ms/sample - loss: 0.0660 - categorical_accuracy: 0.4212 - my_metric: 0.0000e+00 - val_loss: 0.2120 - val_categorical_accuracy: 0.3241 - val_my_metric: 0.0000e+00\n",
      "Epoch 6/20\n",
      "5522/5522 [==============================] - 1390s 252ms/sample - loss: 0.3610 - categorical_accuracy: 0.1092 - my_metric: 0.0000e+00 - val_loss: 0.3553 - val_categorical_accuracy: 0.1710 - val_my_metric: 0.0000e+00\n",
      "Epoch 7/20\n",
      "5522/5522 [==============================] - 1388s 251ms/sample - loss: 0.3183 - categorical_accuracy: 0.1691 - my_metric: 0.0000e+00 - val_loss: 0.3280 - val_categorical_accuracy: 0.1124 - val_my_metric: 0.0000e+00\n",
      "Epoch 8/20\n",
      "5522/5522 [==============================] - 1391s 252ms/sample - loss: 0.2981 - categorical_accuracy: 0.1753 - my_metric: 0.0000e+00 - val_loss: 0.3208 - val_categorical_accuracy: 0.1417 - val_my_metric: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d89cbb0d88>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids],\n",
    "    train_labels,\n",
    "    validation_data=(\n",
    "        [dev_input_ids, dev_input_masks, dev_segment_ids],\n",
    "        dev_labels,\n",
    "    ),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[my_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e3576e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(r'C:\\Users\\Anna\\Files\\SentimentAnalysis\\ugam\\models\\bert2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dd3ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r'C:\\Users\\Anna\\Files\\SentimentAnalysis\\ugam\\test.csv')\n",
    "test_text = test['Review'].tolist()\n",
    "test_text = [\" \".join(t.split()[0:max_seq_length]) for t in test_text]\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a650bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = convert_text_to_examples(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57a8fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 2631/2631 [00:01<00:00, 2456.19it/s]\n"
     ]
    }
   ],
   "source": [
    "(test_input_ids,\n",
    "test_input_masks,\n",
    "test_segment_ids) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d201649",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([test_input_ids,\n",
    "test_input_masks,\n",
    "test_segment_ids], batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0636fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=pred, columns=[['Components', 'Delivery and Customer Support',\n",
    "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
    "       'Installation', 'Material', 'Price', 'Quality', 'Usability',\n",
    "       'Polarity']])\n",
    "df.to_csv(r'C:\\Users\\Anna\\Files\\SentimentAnalysis\\ugam\\results\\bert3_predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3394143",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_binary = np.where(pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bed51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data=pred_binary, columns=[['Components', 'Delivery and Customer Support',\n",
    "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
    "       'Installation', 'Material', 'Price', 'Quality', 'Usability',\n",
    "       'Polarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62d55052",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(r'C:\\Users\\Anna\\Files\\SentimentAnalysis\\ugam\\results\\res_bert3_05.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "726a7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_binary = np.where(pred > 0.3, 1, 0)\n",
    "results = pd.DataFrame(data=pred_binary, columns=[['Components', 'Delivery and Customer Support',\n",
    "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
    "       'Installation', 'Material', 'Price', 'Quality', 'Usability',\n",
    "       'Polarity']])\n",
    "results.to_csv(r'C:\\Users\\Anna\\Files\\SentimentAnalysis\\ugam\\results\\res_bert3_03.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c91ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_binary = np.where(pred > 0.05, 1, 0)\n",
    "len(pred_binary)\n",
    "results = pd.DataFrame(data=pred_binary, columns=[['Components', 'Delivery and Customer Support',\n",
    "       'Design and Aesthetics', 'Dimensions', 'Features', 'Functionality',\n",
    "       'Installation', 'Material', 'Price', 'Quality', 'Usability',\n",
    "       'Polarity']])\n",
    "results.to_csv(r'C:\\Users\\Anna\\Files\\SentimentAnalysis\\ugam\\results\\res_bert3_005.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1dfc1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2631"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987a4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
